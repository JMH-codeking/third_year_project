{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "data_label = list() \n",
    "\n",
    "'''turn string into 0 and 1\n",
    "'''\n",
    "def demap(_list: List):\n",
    "    final = list()\n",
    "    final.append(float(_list[0]))\n",
    "    mid_term = [0.01 if _l=='None' else 0.99 for _l in _list[1:]]\n",
    "    final.extend(mid_term)\n",
    "    return final\n",
    "\n",
    "def reform_data(inputlist: List):\n",
    "    _l = int(len(inputlist) / 10)\n",
    "    len_input = _l * 10\n",
    "    return inputlist[:len_input]\n",
    "\n",
    "'''split dataset into samples for model training\n",
    "'''\n",
    "\n",
    "def data_split(inputlist: List):\n",
    "    len_samples = int (len(inputlist) / 10)\n",
    "    final_samplelist = list()\n",
    "\n",
    "    for i in range(len_samples):\n",
    "        sample = inputlist[10*i:10*(i+1)]\n",
    "        final_samplelist.append(sample)\n",
    "\n",
    "    return final_samplelist\n",
    "\n",
    "def mix_datasets(\n",
    "    input_data: List,\n",
    "    labels: List\n",
    "):\n",
    "    final_output = list()\n",
    "    '''change the datalist into list of dict, then shuffle\n",
    "    '''\n",
    "\n",
    "    for _data, _label in zip(input_data, labels):\n",
    "        data = {\n",
    "            tuple(_data): _label\n",
    "        }\n",
    "        final_output.append(data)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "def test_data_gen(\n",
    "    _l: List,\n",
    "    percentage: List\n",
    "):\n",
    "    return _l[int(len(_l)*percentage[0]): int(len(_l)*percentage[1])]\n",
    "\n",
    "'''label of all data, whole sample data\n",
    "'''\n",
    "\n",
    "labels = list() \n",
    "final_samples = list()\n",
    "\n",
    "sample_list = list()\n",
    "\n",
    "test_list = list()\n",
    "test_labels = list()\n",
    "\n",
    "\n",
    "for a in range (1,100):\n",
    "    with open (f'../datafile/datafile{a}.txt') as _f:\n",
    "        data = _f.read().split('\\n')\n",
    "    data_label.append(data[0])\n",
    "    data = demap(data[:-2])\n",
    "\n",
    "    real_data = data[1:]\n",
    "    len_real_data = len(real_data)\n",
    "\n",
    "    # print (f'''type of len: {\n",
    "    #     type(len_real_data)}\\ntype of data[0]: {type(data[0])}\n",
    "    # ''')\n",
    "    failure_node = int(len_real_data * data[0])\n",
    "    \n",
    "\n",
    "    normal_data = real_data[:failure_node]\n",
    "    fading_data = real_data[failure_node:]\n",
    "\n",
    "    test_list.extend(data_split(reform_data(normal_data)))\n",
    "    test_labels.extend([0] * len(reform_data(normal_data)))\n",
    "\n",
    "    _fail = reform_data(\n",
    "            test_data_gen(\n",
    "                fading_data,\n",
    "                [0, 0.1]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    test_list.extend(\n",
    "        data_split(\n",
    "            _fail\n",
    "        )   \n",
    "    )\n",
    "\n",
    "    test_labels.extend(\n",
    "        [1] * len(\n",
    "            reform_data(\n",
    "                _fail\n",
    "            )\n",
    "        )    \n",
    "    )\n",
    "\n",
    "\n",
    "    # print (fading_data)\n",
    "    normal_data = normal_data[:len(fading_data)] # make the dataset balanced\n",
    "    '''make the data consisted of data having the integer number of hundreds\n",
    "\n",
    "    Label the datasets and create the whole dataset\n",
    "    for training and testing\n",
    "    '''\n",
    "\n",
    "    normal_data_list = reform_data(normal_data)\n",
    "    fading_data_list = reform_data(fading_data)\n",
    "    normal_samplelist = data_split(normal_data_list)\n",
    "    fading_samplelist = data_split(fading_data_list)\n",
    "\n",
    "\n",
    "    '''label the data\n",
    "    '''\n",
    "\n",
    "    sample_list.extend(normal_samplelist)\n",
    "    sample_list.extend(fading_samplelist)\n",
    "        \n",
    "    labels.extend([0] * len(normal_samplelist))\n",
    "    labels.extend([1] * len(fading_samplelist))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "final_list = mix_datasets(sample_list, labels)\n",
    "\n",
    "#\n",
    "test_list = mix_datasets(test_list, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''demap the dict list\n",
    "'''\n",
    "\n",
    "\n",
    "data_sets = list()\n",
    "train_datasets = final_list[:int(len(final_list) * 0.9)]\n",
    "random.shuffle(train_datasets)\n",
    "\n",
    "random.shuffle(test_list)\n",
    "\n",
    "def fig_extract(\n",
    "    final_list: List\n",
    "):\n",
    "    data_sets = list()\n",
    "    for _key in final_list:\n",
    "        data_sets.extend(list(_key.keys()))\n",
    "    data_sets = np.array(data_sets)\n",
    "\n",
    "    data_labels = list()\n",
    "\n",
    "    for _k in final_list:\n",
    "        data_labels.extend(list(_k.values()))\n",
    "    \n",
    "    data_labels = np.array(data_labels)\n",
    "\n",
    "    return data_sets, data_labels\n",
    "\n",
    "X_train, y_train = fig_extract(train_datasets)\n",
    "\n",
    "X_test, y_test =  fig_extract(test_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       ...,\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       ...,\n",
       "       [0.99, 0.99, 0.99, ..., 0.01, 0.99, 0.99],\n",
       "       [0.99, 0.99, 0.99, ..., 0.99, 0.99, 0.99],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# 在此数量的单词之后剪切文本（取最常见的 max_features 个单词）\n",
    "maxlen = 10\n",
    "batch_size = 16\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64))\n",
    "model.add(LSTM(10, dropout=0.5, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# 尝试使用不同的优化器和优化器配置\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
