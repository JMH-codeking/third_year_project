{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/lnt43mls129cb8n4b_0_fkhr0000gn/T/ipykernel_32183/1588373577.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(dataX), np.array(dataY)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/lnt43mls129cb8n4b_0_fkhr0000gn/T/ipykernel_32183/1588373577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "for i in range (155,255):\n",
    "    with open (f'../datafile_new/datafile{i}.txt') as _f:\n",
    "        data = _f.read().split('\\n')\n",
    "        _d = data[1:-1]\n",
    "    \n",
    "    data_list.extend(_d)\n",
    "\n",
    "final_list = list()\n",
    "for i in range (int(len(data_list)/100)):\n",
    "    _d = data_list[100*(i-1) : 100*i]\n",
    "    loss_packets = len([_data for _data in _d if _data == 'None'])\n",
    "    final_list.append(loss_packets)\n",
    "final_list = np.array(final_list)\n",
    "print(len(final_list))\n",
    "\n",
    "\n",
    "dataset = final_list.astype(np.float32)\n",
    "max_value = np.max(dataset)\n",
    "min_value = np.min(dataset)\n",
    "scalar = max_value - min_value\n",
    "dataset = list(map(lambda x: x / scalar, dataset))\n",
    "n = 2\n",
    "\n",
    "def create_dataset(dataset, look_back=n):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# 创建好输入输出\n",
    "data_X, data_Y = create_dataset(dataset)\n",
    "\n",
    "train_size = int(len(data_X) * 0.7)\n",
    "test_size = len(data_X) - train_size\n",
    "train_X = data_X[:train_size]\n",
    "train_Y = data_Y[:train_size]\n",
    "test_X = data_X[train_size:]\n",
    "test_Y = data_Y[train_size:]\n",
    "\n",
    "train_X = train_X.reshape(-1, 1, n)\n",
    "train_Y = train_Y.reshape(-1, 1, 1)\n",
    "test_X = test_X.reshape(-1, 1, n)\n",
    "\n",
    "train_x = torch.from_numpy(train_X)\n",
    "train_y = torch.from_numpy(train_Y)\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "test_loss = list()\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layer):\n",
    "        super(lstm,self).__init__()\n",
    "        self.layer1 = nn.LSTM(input_size,hidden_size,num_layer)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.layer2 = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x,_ = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        s,b,h = x.size()\n",
    "        x = x.view(s*b,h)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(s,b,-1)\n",
    "        return x\n",
    "output = 2\n",
    "model = lstm(n,4,2,3)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# 开始训练\n",
    "for e in range(500):\n",
    "    var_x = Variable(train_x)\n",
    "    var_y = Variable(train_y)\n",
    "    # 前向传播\n",
    "    out = model(var_x)\n",
    "    loss = criterion(out, var_y)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    num_correct = 1\n",
    "    num_correct = torch.eq(pred, var_y).sum()\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (e + 1) % 1 == 0: # 每 100 次输出结果\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.item()))\n",
    "    test_loss.append(loss.item())\n",
    "\n",
    "torch.save(model, f'./packetloss_net_{n}.pkl')\n",
    "    \n",
    "model = model.eval() # 转换成测试模式\n",
    "\n",
    "data_X = data_X.reshape(-1, 1, n)\n",
    "data_X = torch.from_numpy(data_X)\n",
    "var_data = Variable(data_X)\n",
    "pred_test = model(var_data) # 测试集的预测结果\n",
    "# 改变输出的格式\n",
    "pred_test = pred_test.view(-1).data.numpy()\n",
    "# 画出实际结果和预测的结果\n",
    "plt.plot(pred_test, 'g', label='prediction')\n",
    "plt.plot(dataset, 'b', label='real')\n",
    "plt.legend(loc='best')\n",
    "plt.title('The graph of prediction and real dataset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEdCAYAAADwwTuSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0A0lEQVR4nO3dd3yV9fn/8dd1ck4WI2GEPQKIKOJAUVCcrbsqWndrHbW1trXVfltbq62j2j1/Wqu1atW2bquC0rrRupChIAjIlA1hBgLZ1++P+w4cTgY3kOSE5P18PO5Hzj3OfV93COc6n3mbuyMiIhJFLN0BiIjI3kNJQ0REIlPSEBGRyJQ0REQkMiUNERGJTElDREQiU9KQZmdmt5qZm9ncevbPC/ffmrL9cjObYmabzGy9mX1oZn9IOcbrWebVc62Ema0zs7saiHeGmY3flTjqOMflYRztw/Vu4e+hsKH3NQUzO9nMrqtj+0NmNrm545G9i5KGpEspMMDMRiRvNLPDgf7h/uTtPwbuB14CvghcCjwPnFXHuX8PHJmynFdXEO5eATwDnG9mGan7zewA4ADgsd2II9mLYRxbwvVuwC1A4U7e1xROBq6rY/vtwOXNGonsdeLpDkDarBJgKnARkPzt9iLgdeCwlOOvAf7q7jcmbRtnZrfVce5F7v7+LsTyGPA14HjgtZR9FxMksOd2I45t3L0IKNqFmHaJmeW4+9Y9OYe7z2+seKT1UklD0ulx4AIzM4Dw5wXh9lT5wMrUjd44UxpMAFYQJKxUFwIvuPumPYkjuXoqrJL6ONz1Rk0VWtKxnc3sr2a2ysxKzexdMxuZcj43s/8zsz+ZWVHN+czsC2b2ipmtNrNiM3vfzE5Oet+twPeB/klVdw+F+2pVT5nZIWb2mpltCavi/mVm3ZP2F4bnuCCMeaOZLTWz28wslnRcHzN7Moxrq5nNN7PbG/qdScukpCHp9G+gO3B0uH4MUAA8W8exU4HvmNllZtZlJ+eNmVk8Zan3b93dq4EngS+aWaJme1h1tg9h1dRuxFGfFcCXw9ffZnsVGmaWBbwKnARcD5xNUEJ51cx6pJzneqAn8BXgu+G2AcC4cNu5wLvAf8xsdLj/fuBRgsRXc906P7zNrIAgoeYCXwK+AxwHvGJmmSmH/wbYTFAN+E/gZnasEnwE6AtcBZwG/BzIquu60sK5uxYtzboAtwJrwtfPA3eHr/8CPBe+XgPcmvSeg4AFgAPVwEzgZ0DHlHN7PctDO4lpZHjcF5K2/Q7YCGTvahx1nP/y8D3tw/Vh4frxKcddCZQDg5O2xYH5wG9T7vPDnVwzFr73JeDBlPtaVMfxDwGTk9Z/BWxIvjfgiPDaF4frheH6Iynn+gh4PGl9M3Bmuv/2tOz5opKGpNvjwHnhN+zzqLtqCnefDuxP0OD8F8CAnwKTa3okJfktcHjKcmtDQbj7RIJkcCHsUFX2rLuXJh23K3HsjhOBKcDCmlJSuP1NYETKsS+mvjmsBnrYzJYBlUAFQcP3vrsRyxHAy+5eXLPB3T8AFrG9dFjj5ZT1T4A+SesfAb8Mq+n67UYs0kIoaUi6jQXaE1RXtCOoWqmTu5e5+zh3v8bdhxI0Xg8m+HaebLG7T05ZFkWI5XFgjJllA0cRVKc8lnrQLsSxO7oCowg+7JOXK8J4kq1KXgmr4MaGsd8MnECQMP8DZO9GLD1Tr5F03c4p2zakrJenXPNCgg4PfwQ+M7OPzOzzuxGTpJmShqSVu5cALwDfA8aF61Hf+wCwDtivkcJ5DOgInE7QKF5E7d5UTR3HOoIP19SS0uHAOamXTlnfBxgOfMfdH3D3N919MpCzm7GsIOganKp7GGdk7r7M3S8HuhC0o6wExu5Bu5CkibrcSktwD0Gj6L31HWBm3dx9dcq2AiCPur8N7zJ3n2FmMwgafUcDT7l7ZRPFUR7+TC0BvEZQnbQ49ToR1CSHsqTY+hPcy/SUa0cpeUwEvmlmHTzsPRaOoykE3t7F2IBtnQ7eD7sov0swJmft7pxL0kNJQ9LO3ScQ9NJpyMdm9jxB3flqgg+bHxAMlns45dhCMxtV+zI+MUI4jwF3ELRV1Kqa2sU4GrIY2ApcZmYbgYqwVPAIcDUwwcx+R9DO0oWgfWGlu/+xgXPOBpYCvzeznwIdgNuAZXUc193MLgdmEHRKWFTH+f4AfBN4ycx+TVCN+CuC7r3PRL1RM8sjaIx/BPiU4AvC9wlKG7OinkdaBiUN2Vv8DBgD3ElQn76S4Jvqhe6+MOXY74dLsiqi/b0/RtC+sgR4Zw/jqJe7l5rZ1wlGhb8JJAALt58QXuc2gqqg1cAHBO0VDZ2zzMy+CNwNPE2QQH5OMGhxWNKhTxK0d/yGoIvzw9QxEtzdi8JYfk/weykHxgPfc/fy1OMbUEqQaK4laJfZArwPnOx7OCBRmp+563GvIiISjRrCRUQkMiUNERGJTElDREQiU9IQEZHIWn3vqa5du3phYWG6wxAR2WtMmTJljbsX1LWv1SeNwsJCJk/Ww8hERKIys8/q26fqKRERiUxJQ0REIlPSEBGRyJQ0REQkMiUNERGJTElDREQiU9IQEZHIlDTqcedrc3nz06J0hyEi0qIoadTjngnzeXuukoaISDIljXrEM4yKKj1rREQkmZJGPRIZMSqrq9MdhohIi6KkUY94zKhUSUNEZAdKGvVIZMRUPSUikkJJox7xDFP1lIhICiWNeqh6SkSkNiWNegTVUyppiIgkU9KoR1A9pZKGiEgyJY16xGMqaYiIpFLSqEciQ20aIiKplDTqEY9pcJ+ISColjXpoGhERkdqUNOoRj2mchohIKiWNesQzYmrTEBFJoaRRj0SGqfeUiEgKJY16xGMxqjROQ0RkB0oa9VBDuIhIbUoa9Uioy62ISC1KGvWIa3CfiEgtShr10ISFIiK1KWnUIxinoZKGiEgyJY16aJyGiEhtzZ40zOxUM5tjZvPM7IY69u9nZu+ZWZmZ/WBX3tuYEhlGhRrCRUR20KxJw8wygLuB04ChwMVmNjTlsHXAd4Hf7cZ7G008FsMdjdUQEUnS3CWNI4B57r7A3cuBx4ExyQe4+2p3nwRU7Op7G1M8wwDUGC4ikqS5k0ZvYEnS+tJwW1O/d5clwqShxnARke2aO2lYHduifipHfq+ZXWVmk81sclFRUeTgksVjwa+mUiUNEZFtmjtpLAX6Jq33AZY39nvd/T53H+HuIwoKCnYr0MS26imVNEREajR30pgEDDazAWaWCVwEjG2G9+6yeEZY0lAPKhGRbeI7O8DMsoAfAC+4+7Q9uZi7V5rZNcBLQAbwoLvPNLOrw/33mlkPYDLQEag2s+uAoe5eXNd79ySehsRjYZuGShoiItvsNGm4e5mZ3QS83RgXdPfxwPiUbfcmvV5JUPUU6b1NJRGWNNR7SkRku6jVUxOBw5oykJZme9JQSUNEpMZOSxqhHwKPmlk5wTf9VaT0XHL3LY0cW1plxlXSEBFJFTVpTAx/3gn8v3qOydjzcFqOmt5TZZVKGiIiNaImja8SfTxFq5CpNg0RkVoiJQ13f6iJ42hxaqqnylXSEBHZJmpJAwAz6wUcCXQmmFjwPXePOjhvr6LeUyIitUVKGuEMs3cBX2fHtosqM7sP+I67t6pPVzWEi4jUFrXL7W0E7Ro3AoVATvjzxnD7rY0fWnrVlDTUEC4isl3U6qlLgZ+4e/IzLhYDvzUzJ3j+xc2NHVw6ZcU1TkNEJFXUkkY3YHo9+6aH+1uVmpKGGsJFRLaLmjQ+JZggsC4XAXMaJ5yWI6GHMImI1BK1euoO4HEz6wc8TTAivBtwPnAC9SeUvZYawkVEaos6TuNJM9tA0CD+/4AEweNYpwCnuvsrTRZhmqghXESktqhTo58HfODuR5pZDOgKrGlt3WyTaUS4iEhtO23TcPcy4H6gV7he7e6rW3PCAIjFjHjM1BAuIpIkakP4x8C+TRlIS5QZj6mkISKSJGpD+PeAh8xsBfBfd69swphajERGTCUNEZEkUZPGc0Au8DzgZrae2s/TaJVjNco1uE9EZJuoSePPTRpFC5Wl6ikRkR1E7T01j6D31NymD6nlSGSoIVxEJNku955qS9QQLiKyI/WeaoAawkVEdqTeUw0IGsKVNEREaqj3VAMy4yppiIgki5o07iYlSbQFWfEYm8vaRKFKRCSSqBMW3trEcbRIWfEY60pU0hARqRG1IbxeZhY3s1bZsyornqFZbkVEktSbNMys3MwOT1qPmdnrZjY45dDDgCVNFWA6ZcVjlFVWpTsMEZEWo6GSRhywpHUDjgc6NGVALUlWIkZZhUoaIiI19rh6qjVT9ZSIyI6aPWmY2almNsfM5pnZDXXsNzO7M9w/3cwOTdr3PTObaWYzzOwxM8tuylhVPSUisqNmTRpmlkHQffc0YChwsZkNTTnsNGBwuFwF3BO+tzfwXWCEuw8DMmjiZ5MHSaMa9zbX21hEpE4763L7nXAUOGxv37jWzFYlHdNzF653BDDP3RcAmNnjwBjgk6RjxgCPePBJ/b6Z5ZtZzTXiQI6ZVRAMNly+C9feZVmJDNyhosrJjNvO3yAi0so1lDQWA0enbPsMOLaeY6PozY49rZYCIyMc09vdJ5vZ78JrbQVedveX67qImV1FUEqhX79+EUOrLSseFMTKKqvIjKv5R0Sk3qTh7oVNcL26vq6n1v3UeYyZdSIohQwANgBPmdkl7v7PWge73wfcBzBixIjdrlvanjSq206XMRGRBjT31+elQN+k9T7UrmKq75gTgYXuXuTuFcC/gaOaMFay4hkAmn9KRCTU3EljEjDYzAaYWSZBQ/bYlGPGApeGvahGARvdfQVBtdQoM8s1MwM+D8xqymAzk0oaIiISfcLCRuHulWZ2DfASQe+nB919ppldHe6/FxgPnE7wtMAtwBXhvolm9jQwFagEPiSsgmoqyW0aIiLSzEkDwN3HEySG5G33Jr124Nv1vPcW4JYmDTBJViJMGhoVLiICaER4g2raNFQ9JSIS2OWkEbY19DKzZi+lNDdVT4mI7Chy0jCz081sIlBK0Ch9ULj9PjO7pIniS6ttJQ1VT4mIABGThpldStCraTbBoLnk980Frmz80NJvW5uGqqdERIDoJY2bgN+6+2VA6mC6mQTzSLU6qp4SEdlR1KTRH3ilnn2lQMfGCadlyU4E1VOlqp4SEQGiJ40lwPB69o0gGFPR6uRkBklja4VKGiIiED1pPADcEjZ454TbzMw+D/wQ+FtTBJduOdtKGkoaIiIQfXDfrwnmg3oYqPkEfZdgVPdf3f3OJogt7RIZMeIxY2u5koaICERMGjWjtM3sDwRzPnUF1gGvu/unTRhf2uUkMlQ9JSIS2mnSCB+puhG40N2fA+Y3dVAtSXZmBltU0hARASK0abh7KbCaYJLANicnkaE2DRGRUNSG8L8C3zWzRFMG0xLlJDLUpiEiEoraEJ4PDAMWmdlrwCp2fOKeu/uPGjm2FiE7U20aIiI1oiaNc4Gy8PUxdex3oFUmjZxETElDRCQUtffUgKYOpKXKSWSwtqQ83WGIiLQIep7GTuRkqk1DRKRG5GdihM/lHg3sC2Sn7nf3vzRiXC1GtsZpiIhsEylpmFl34DWC2WwdsHBXcmN4q0wauZnqcisiUiNq9dTvCQb49SVIGCOBQuCnBM/T2LcpgmsJ1OVWRGS7qNVTxwHXAivCdXP3xcAvzCxGUMo4pQniS7ucRAZbKqpwd4IaOhGRtitqSSMfKHL3aqAY6Ja0713gqEaOq8VolxXHXdOji4hA9KSxEOgZvp4JfDlp35kEkxe2SrlZQWFsc1mbnEVFRGQHUaunXgROBp4E7gCeN7OlQAXQj1Y6sA+gfVbwTI0tZVXQIc3BiIikWdTBfT9Oev0fMxsNnE3wQKZX3P0/TRNe+uVmqqQhIlIj8jiNZO4+CZjUyLG0SO3D6ilNjy4iEn2cxtCdHePun+x5OC1Pbvic8BKVNEREIpc0ZrDjQL66ZOxhLC1STUmjpFxJQ0QkatI4oY5tnQkax08mGMPRKtX0nlJJQ0QkYpdbd3+zjuVZd/8m8BhwQdQLmtmpZjbHzOaZ2Q117DczuzPcP93MDk3al29mT5vZbDObZWZHRr3u7mqfWZM01KYhItIYs9y+AYyJcqCZZQB3A6cRzGN1cR3tJacBg8PlKuCepH3/D/ivu+8HHAzM2rPQdy43S20aIiI1GiNpfAHYEPHYI4B57r7A3cuBx6mdcMYAj3jgfSDfzHqaWUfgWOABAHcvd/eo191tiYwYmfEYm9WmISISuffUk3VszgT2IygR3Bjxer2BJUnrSwkmP9zZMb2BSqAI+LuZHQxMAa5195I64r2KoJRCv379IoZWv/ZZcZU0RESIXtIoqGPJAv4HnOnuv454nrpm/EvtlVXfMXHgUOAedx8OlAC12kQA3P0+dx/h7iMKCgoihla/DtlxircqaYiIRB0RXlfvqd2xlGB69Rp9gOURj3FgqbtPDLc/TT1Jo7Hl5STYuLWiOS4lItKiNffjXicBg81sgJllAhcBY1OOGQtcGvaiGgVsdPcV7r4SWGJmQ8LjPg80y4BCJQ0RkUDUNo0Hd+Gc7u5X1rOj0syuAV4iGAz4oLvPNLOrw/33AuOB04F5wBbgiqRTfAf4V5hwFqTsazIdcxIsW7+1OS4lItKiRR3cdyBBlVE3YHW4dAuXImBx0rENjhx39/EEiSF5271Jrx34dj3v/QgYETHmRqOShohIIGr11M8IGp6Pdvce7n6Qu/cAjgE2Abe7++HhckRTBZsueTkJiksrCPKZiEjbFTVp/Ar4ibu/m7zR3d8Bbgai9p7aK+XlJKiocj29T0TavKhJYyBB+0JdtgCFjRJNC5WXkwBQFZWItHlRk8ZU4FYz65m80cx6AbcSDLRrtWqSxoYtShoi0rZFbQi/CngZWGRmU9jeEH4YsBa4pGnCaxm6tMsEYO3m8jRHIiKSXlFnuZ0JDAK+B8whGA0+J1wf5O4zmizCFqCgQxYARZtL0xyJiEh6RX7cq7uXAn9pwlharG4dswFYXVyW5khERNJrt54RbmZnEkxWuBJ4zt03NWpULUz7rDi5mRms3qSkISJtW71Jw8x+BJzh7sckbUsArwGj2T6x4BIzO9LdU+eQalUKOmQpaYhIm9dQm8Y5wDsp274LHA3cAXQkGJ1dBdzUJNG1IN06ZLG6WG0aItK2NZQ0BgHvp2y7CFjo7re4+2Z3n0ow8O+kpgqwpeiRl8PyjZp/SkTatoaSRi5JT+Qzs/bAcODVlONmEzwkqVUb0CWXZeu3UlapUeEi0nY1lDQWEDyetcZJBO0Yr6UclwcUN3JcLU5h13ZUOyxZV9/AeBGR1q+h3lN/JxgFXgmsAm4jmNH2xZTjTiAYs9GqFXZtB8DCNVvo0ymXeas3M3f1JgZ368Cw3nlpjk5EpHk0lDTuBIYAvwQSBM/tvjj5mdxmlgdcRiufsBBgUEF7YgZPTFrCNY9OpayyGoCBXdvx+g+OT29wIiLNpN7qKXevdPdvAPlAN3fv7+5vpBxWAuwL/LHpQmwZ8nISHDWoK6/OWkVZZTV3XTycMYf0YsGaEuatbtXDVEREttnpNCLuvtXd19Szr9Ld17p7m5jJ77KjCgHYv2dHzjy4Fzeevj9mMG7aivQGJiLSTJr7GeF7tZOGdufpq4/kvq8cBkD3jtkcUdiZcdOW6wFNItImKGnsohGFnenbOXfb+vkj+rJgTQnvzFubxqhERJqHksYeOuOgnnTvmMUdL35Cedg4LiLSWilp7KHsRAY/P/tAZq/cxJ9fn5vucEREmpSSRiM4cWh3vnhob+6eMJ9H3luk9g0RabUiT41uZucBXwT6ANmp+939iFpvakNuOeMAlq7bys3Pz+SdeWu4YERfRu/TlexERrpDExFpNJGShpndCtwMTAM+AfTc0xR5uQme+MYo/jJhPvdMmM9LM1eRl5PgrIN7cfqBPRk5oDOxmO38RCIiLZhFqUoxsyXAP9z9xqYPqXGNGDHCJ0+e3KzXLKus4v0F63jonYW8O38tZZXVHNw3n1+ecyBDe3Vs1lhERHaVmU1x9xF17YtaPdWB2hMVSj2y4hkct28Bx+1bwPqScn71n9k8MXkJ597zLl87ZgAXHdGP3vk56Q5TRGSXRW0Ifxw4tSkDaa06tcvkV+ceyKv/dywH9s7jrtfnccG977F8g57NISJ7n6jVU+cTTEr4JvAKSc/ZqOHu4xs7uMaQjuqphny4eD2XPvAB2ZkZfPv4QVw+ekC6QxIR2UFjVE89Ef4sJJjVNpUD6iYUwfB+nXjiG0fy439P59Zxn9ClfRZnHtwr3WGJiEQStXpqwE6WgVEvaGanmtkcM5tnZjfUsd/M7M5w/3QzOzRlf4aZfWhmL0S9ZksztFdHnv7mURzSN58bn/2YSYvWpTskEZFIIiUNd/9sZ0uU85hZBnA3cBowFLjYzIamHHYaMDhcrgLuSdl/LTAryvVaskRGjLsuHk6Xdpl87eHJrCouTXdIIiI7tUsjws0sbmYDzWxo6hLxFEcA89x9gbuXEzSwj0k5ZgzwiAfeB/LNrGd4/T7AF4D7dyXulqpv51wevPxwSiuquOnZj9MdjojITkVKGmaWMLN7CJ4FPhf4uI4lit4ETwCssTTcFvWYPwE/BBqcGdDMrjKzyWY2uaioKGJo6TGwoD3XnjiYV2etZsayjekOR0SkQVFLGjcDZwBXAgZcA1xBMHZjEXBmxPPUNSQ6tftWnceY2RnAanefsrOLuPt97j7C3UcUFBREDC19vjyyPzmJDP72vwXpDkVEpEFRk8YFwK3Ak+H6B+7+iLufDLxN7Sqm+iwF+iat9wGWRzxmNHCWmS0iqNb6nJn9M+J1W7S8nARXjC7k+Y+WM3tlcbrDERGpV9Sk0Rf41N2rgFKgU9K+fwHnRjzPJGCwmQ0ws0zgImBsyjFjgUvDXlSjgI3uvsLdf+zufdy9MHzf6+5+ScTrtnhXHTuQzHiMRycuTncoIiL1ipo0VgD54euFwLFJ+wZFvZi7VxJUbb1E0APqSXefaWZXm9nV4WHjgQXAPOBvwLeinn9vlp+byRcO7MmzU5expbwy3eGIiNQp6uC+CcAxwDiCD/Lfmdk+QBlwIfBY1AuGI8fHp2y7N+m1A9/eyTkmhDG1Khcf0Y9nP1zG+I9Xct5hfdIdjohILVFLGjcBjwC4+5+A64H+wMHAXcB3myK4tubwwk706JjNq5+sSncoIiJ1ilTScPeVwMqk9T8Cf2yqoNoqM+OE/QoYN20F5ZXVZMb1YEURaVl2dXDfUDP7ipndaGY9wm37mFmHpgmv7Tl+SDc2l1Uy+TNNLSIiLU/UwX3tzexJgkF89wO3AzWz7P0CuKVpwmt7Ru/TlUSGMWFOyx6UKCJtU9SSxh+Ao4ATCR7IlDwAbzx61kajaZ8VZ+SALrwxe3W6QxERqSVq0vgi8CN3fwOoStn3GUGjuDSS44cUMHf1Zpas25LuUEREdhA1aeQAa+vZ14HaiUT2wCkH9CBm8Mh7i9IdiojIDqImjUnApfXsOw94t3HCEQhmvz11WA+embqMKE9WFBFpLlGTxk+AL5rZq8DXCCYZPN3M/gGcjxrCG92RA7uwrqScZXqWuIi0IFEfwvQ28HkgC/gzQUP4bQRP7DvR3Sc1WYRt1EF98gH4eKmmSxeRliPyOA13f8fdjwE6Esw828HdR7v7O00WXRu2X88OJDKM6XrGhoi0IFHnntrG3bcCqjNpYlnxDIb06KCShoi0KPUmDTO7eRfO4+5+eyPEI0kO6pPPC9OW4+6Y1fVsKhGR5tVQSeNWghJFCXU/TS+ZE4wSl0Y0ckBnHp24mGemLtOstyLSIjTUprEASABTgB8Ag9y9oJ6lW7NE28aceVAv9uvRgacmL9n5wSIizaDepOHu+xBMHTKToBSx0sz+bWbnm1lOcwXYlsVixrDeeSxaW5LuUEREgJ30nnL3ye7+A3fvRzC/1EqCLrerzexfZnZsQ++XPTegaztWFZdRUqan+YlI+u1Kl9u33P1bBM8Lv5fgiX3XNVFcEirs0g5ApQ0RaREid7k1s9HARQTThnQAngbuaaK4JDSkR/Coklc+WUWn3EwAeuWrdlBE0qPBpGFmhxIkiguB7sB/ge8BY91dU7A2g326tee0YT24+415/OnVuXRpl8mUn56U7rBEpI2qt3rKzOYA7wMHEcwt1c3dz3b3x5UwmtcFI/pSURVMXLi2pJwXp69g7qpNaY5KRNqihkoag4FS4DDgUOA3DQ0wU7fbpnPkoC5kxWOUVVYD8O1Hp9KnUw5v/+hzaY5MRNqahpLGbc0WhTQoO5HBBzeeyOTP1nHlw5MBWLp+K29+WsRx+xakOToRaUvqTRrurqTRguTlJjiwTx4ZMaOqOqiquuzBD1j0qy+kOTIRaUsid7mV9OvWIZtZPzuVm88Yum3bio2aO1JEmo+Sxl4mMx7jy6P6ceXRAwCY8tn6NEckIm2JksZeKCuewQ2n7UeHrDhvz12T7nBEpA1R0thLJTJijN6nKxPmFFFWWZXucESkjVDS2ItdcHgfVhaX8oOnplMdNo6LiDSlZk8aZnaqmc0xs3lmdkMd+83M7gz3Tw9HpWNmfc3sDTObZWYzzeza5o69pfncft25/pQhjJu2nDc/LUp3OCLSBjRr0jCzDOBu4DRgKHCxmQ1NOew0goGFg4Gr2D6/VSXwfXffHxgFfLuO97Y5Xz9mIGZwxUOTeHee2jdEpGk1d0njCGCeuy9w93LgcWBMyjFjgEc88D6Qb2Y93X2Fu08FcPdNwCygd3MG3xJlxmNcMrI/AD95bsa2MRwiIk2huZNGbyD5MXRLqf3Bv9NjzKwQGA5MrOsiZnaVmU02s8lFRa2/2ua2sw7g9+cfzII1JXy0ZEO6wxGRVqy5k0Zdk1elfjVu8Bgzaw88A1zn7sV1XcTd73P3Ee4+oqCg9U+zEYsZJ+7fnYyYcdOzH3PJ/RN5e+4aPl21iWemLE13eCLSikR+nkYjWUrwEKcafYDlUY8xswRBwviXu/+7CePc6+TlJrhkZD/enb+Wj5dt5JIHthfCjhtSQNf2WWmMTkRai+YuaUwCBpvZADPLJHhWx9iUY8YCl4a9qEYBG919hQVT7D4AzHL3PzRv2HuH28YM45X/O45bz9qxf8Abs1enKSIRaW2aNWm4eyVwDfASQUP2k+4+08yuNrOrw8PGAwuAecDfgG+F20cDXwE+Z2YfhcvpzRn/3uKc4X344MbPc+mR/cmMx7jvrQVsLa/ikfcWadoREdkj5t66e9uMGDHCJ0+enO4w0uaNOav56kOTqPlnbp8VZ8Ztp6Q3KBFp0cxsiruPqGufRoS3cicM6ca1nx+8bX1zWSVn/fltLrl/Io+8t4it5ZqCRESiU9JoAy49spD9e3bkjxceDMD0pRt5e94abn5+Jif8boISh4hEpqTRBnRul8l/rj2Gc4b3YXC39gC8c8Pn6NYhi5XFpTz4zkJKK5Q4RGTn1KbRxhSXVlBeWb2tC+5XHpjI/+auITMjxg9PHUIiI8a6knK+NLIf3TtmpzlaEUmHhto0mnuchqRZx+zEDus/PGU/MjM+ZdrSjdzx4qxt299fsJaHv3oE2YmM5g5RRFowlTQEgKJNZbzyySr279mB12ev5q7X523bd9Pp+/P1YwemMToRaU4qachOFXTI4ksj+wHQMy9nh6Tx8/GzqKx2Duydx9GDu6YrRBFpAZQ0pJYeedlcf8oQDuydx/B++Rz7mzf49X9nA/C1owdwzqG9OaBXHms2l3HF3yfx1aMLOWd4nzRHLSLNQdVTslNL129h8qL1PPbBYiYuXEdmRoz83ASZ8RhL12/FDH5//sEc0jefVz5ZRW5m0A7ylSMLt53D3TGzbT8bMntlMfk5mfTI23lDfHW18+qsVXxuv27EM3a/M+Dbc9dw+IBOZMUbrw2ntKIKMxrlnO7O6k1lLaZzQnW188GidYwc0Hmn/57Jovz716e0okptbM2koeopJQ3ZJZ+tLeH6p6bzwaJ1APTOz2FA13a8M38NqX9K/bvkcty+BYwo7MzNz8/gn1eO5Pqnp3PWwb0477A+mMEj733GPt3aM3fVJuYXbebgPvn88j+z6dYhi+tPGcKitSWcf1hfbh47k/mrN/P4VaPo2zkXCD64nv1wGd9/ahqXH1XI144ZwI+emc5XRhXy2doSvjyqP+0yM3b4kJry2XqWrNtC/y655GRmsF+Pjrw3fy0X/+19vvu5fSjs2o71Wyq48ugBQPAht7aknC7tMuv8sCuvrKa4tIKu7bMorajiw8UbOLBPHovWlHDuPe/SKTeTm88cysF98+nZMZtYLDjH1MXr6dspl4IOO04k+fHSjQwoaEf7rDjuzoI1JQzs2o47XpzFA28v5JlvHslh/Tvv8J4ZyzYyfelGTtivgB4ds3eIs6KqmgwzYjHjngnz6dIukwsO70td3J11JeW8Nms1x+zblZ55OfX+HTz+wWJu+PfHXH5UId84biCdcjO3faDPXL6RWSs2MXqfLjuc487X5jJu2nLGXnM0ZZVVTPlsPfm5iW3388L05bw8cxW/Oe8g5qzcROd2mfzqP7NZs7mMk4Z2544XZ/GPK4/gmMHbZ66urnbMaDARbdhSTl5O0AGk2mFVcSkdsuNkxIwnJy3hsP6dmfLZOk4c2p0+nXK3ve/9BWvZsKWCk4d254nJSzhtWA/yczPZuLWC/80t4vgh3Wiftb2yZuKCtZRXVTPls/VcMqo/7TLjVLtT7c74j1dw9vDezFu9mY7ZiW1/wwDzVm8ikRFjS3kVA7q2o6yyelu8damudiqrncx4jFXFpWwtr6Kwa7sd/h13NzHXUNJQ0mh0W8oreXTiYob26sih/Trxw6en88L05Zx3WB8O7pvPHS/MYmvK2I/sRIzSiuo9uu5h/TtRUlbJio2lbNxasdPjjxjQmYeuOJwPF2/glU9W8dC7i3bYf9WxA1m6fgvjP15Jp9wE67cE5+zcLpOYGVnxGMs2bAXgpKHdaZeZQbeO2bTPipOdiPHU5KXMXb2ZDtlx8nMTLFm3td5YhnTvwMNfPYI1m8s44663ATj30D5UVFXTu1MO90yYv+3YK0YX0q9zLreN+4RRAzvz/oJ12/Z1bZ/FmEN68conqwBYvG7Ltn2j9+lCZVXwoXLpkf256dkZbC6rpKBDFkWbygA46+BeXHZUf3763Ew6tUuwqbSSO84exp9encvrSZNbnjS0O+WV1cRjxsriUjrlZrKyuJQzDurJn16du8O9Hdovn02lleTnJpi0aPv8Zj8bcwAXjOjLrBXFnPOXd+v8vVwxuhCAv7+zKLy/TNZsLq/z2PzcBB2zE8QzjO+duC8Pv7uIxeu2cNlRhQzrnUdOIoNx05Zz6rAebNxawdTP1nP/2wsp7JJLPCNGRVU1S9ZtIS9n+791jWG9O/L9k4YwZ9UmDuydx5fvD2aKPqRvPh8t2UDndpn85Av7M23JBh5+7zMg+GJ0UJ98Du2Xz23jPqkVb8fsOB1zEixdv5UBXduxcE0J+bkJvnncIByoqKzm9698Wue97tu9PRVVTnllNece2pv9e3akstr5zmMfAsEXtpq/zUe/NpJZKzexvqSc+99ewNmH9Obcw/pweGHnOs+9M0oaShrNoqrayQi/SRdtKmPWimK2VlRx+wufkJ+bYMayYs4+pBcbtlYwYU4R2YkYPzp1P4Z078AHi9YxZ+UmXp+9mq8ePYAnJi1hXcn2D46XrjuWsdOWcfcb88nLSURKGLtjUEE7+nXOZc7KTSzfWArAfj06MHvlpjqP79Iuk7VJcY45pBfPf7TjbP/ts+JcMbqQB95eSGV18CGQrKH7qUlkHbPjFJdW7hBPPGZkxIyyyj1LxDVyMzPY0sDsAPGYUbkHT4Zsl5lBSQPnH9qzI0cP7sq4actZEf7u63Jov3ymLt4Q+br5uQk2JCWIjJhxWL9O20rLjenkod2ZtnQDq4rLtm0b1rsjM5YFj/7p3jGLNZvLd3jCZn3//l3aZTJqYBdmryxmflHJLsfSMTvO1J+etFvVtkoaShppV13tvDW3iFEDu5CdyMA9+DacSPmDTq5u2FJeyZ2vBb24bjhtPwBWbiylS/tMlqzbwtzVm1lXUs7R+3Rl5vJiZizbyNXHD6Kq2tmwpZzKaue1WatYV1JB70454M64aSs4b0QfjhrUhfLKar75z6nMWbU9Ibx03bEM6dEBgLmrNrGquIyRAzvz3xkrWbSmhOOHdGNTWQVzVm7iqEFd6dc5l588N4Ph/fK5YERfMuMxVheXMnHhOh6duJj3Fqzln1eO5OjBXXl55kr++tYC9u3enuOHdOPhdxexb/cO3HLmUFZsLKVbhywyYsbS9Vt54O2FjJu2nH99fSQAXdplsbakjLWbyzHgS/dP5NfnHshRg7ryj/c/46A+eRxe2Jlnpi7l1U9W0a9zLs99tJzrTxnCN48bxF/fWsBRg7qwfMNWVhWX8sacIsorqzmobx7L1m9l8qL1/PlLw+ncLpMNWyso7NKOVz5ZyYeLN3DO8N5kJTI4qHces1YW84vxs3hn3lrOP6wP158yhPbZcR56dxHnHhpUOW4qraRnXjavzVrN2GnL6ZWXTY+8HA7r34nuHbPIjMfITcTJzowx9qPl9MzLYfbKYi4Z1X9bFZe7s6W8CgfKKqq47omP6JWXw5dG9uPgvvk8OWkJf31rPqcN68k+3drz3EfLuHBEX96au4ahPTtQXFrJm58W8cHCdbz+/eP4zX/ncNqBPejfpR3uzvB+nVhQtJmczAx+MX42F47oyzvz11DtToesOA+8vZAfn7Y/B/fNZ8m6LVRWO/OLNjO4W3tuf/ETDuqTz4vTV3D72cMY2rMD597zHgCzbz+VhWtKeOvTIgYVtOeE/bqRETMWrilh0doSjhtcQHlVNSVllXy6ajNfuv997vnyYRRvraC0sore+Tm0y4pT0CGLQQXB7A1L12/h6SlLGdytA99+dCrnDO/NxUf046WZK7nx9P056Y9vsqCohJ+NOYBx05Zz5dEDKd5awf49OzKsd8fdqqpS0lDSkHqUVVbx/oJ17NOtPTOXbeTkA3qkOyQg+NCsqArqreuyfMNWeuXX3+ZQXe3MWbWJwd3a7/SbZpR2gVTJpcqWqqaNpksTPIDM3Xlr7hqO2acrsZgxdfF6KiqrGTmwyy6dZ2t5FTmZ0Rv35xdtpntYPVpj1opixn+8gv87ad89bsuooaShpCEiEpmmRhcRkUahpCEiIpEpaYiISGRKGiIiEpmShoiIRKakISIikSlpiIhIZEoaIiISWasf3GdmRcBnu/n2rsCaRgxnb6B7bht0z23D7t5zf3cvqGtHq08ae8LMJtc3KrK10j23DbrntqEp7lnVUyIiEpmShoiIRKak0bD70h1AGuie2wbdc9vQ6PesNg0REYlMJQ0REYlMSUNERCJT0qiDmZ1qZnPMbJ6Z3ZDueBqLmT1oZqvNbEbSts5m9oqZzQ1/dkra9+PwdzDHzE5JT9R7xsz6mtkbZjbLzGaa2bXh9lZ732aWbWYfmNm08J5vC7e32nuuYWYZZvahmb0QrrfqezazRWb2sZl9ZGaTw21Ne8/uriVpATKA+cBAIBOYBgxNd1yNdG/HAocCM5K2/Qa4IXx9A/Dr8PXQ8N6zgAHh7yQj3fewG/fcEzg0fN0B+DS8t1Z734AB7cPXCWAiMKo133PSvf8f8CjwQrjequ8ZWAR0TdnWpPeskkZtRwDz3H2Bu5cDjwNj0hxTo3D3t4B1KZvHAA+Hrx8Gzk7a/ri7l7n7QmAewe9mr+LuK9x9avh6EzAL6E0rvm8PbA5XE+HitOJ7BjCzPsAXgPuTNrfqe65Hk96zkkZtvYElSetLw22tVXd3XwHBByzQLdze6n4PZlYIDCf45t2q7zuspvkIWA284u6t/p6BPwE/BKqTtrX2e3bgZTObYmZXhdua9J7jexBsa2V1bGuL/ZJb1e/BzNoDzwDXuXuxWV23Fxxax7a97r7dvQo4xMzygWfNbFgDh+/192xmZwCr3X2KmR0f5S11bNur7jk02t2Xm1k34BUzm93AsY1yzypp1LYU6Ju03gdYnqZYmsMqM+sJEP5cHW5vNb8HM0sQJIx/ufu/w82t/r4B3H0DMAE4ldZ9z6OBs8xsEUGV8ufM7J+07nvG3ZeHP1cDzxJUNzXpPStp1DYJGGxmA8wsE7gIGJvmmJrSWOCy8PVlwPNJ2y8ysywzGwAMBj5IQ3x7xIIixQPALHf/Q9KuVnvfZlYQljAwsxzgRGA2rfie3f3H7t7H3QsJ/s++7u6X0Irv2czamVmHmtfAycAMmvqe09363xIX4HSCXjbzgZvSHU8j3tdjwAqgguBbx5VAF+A1YG74s3PS8TeFv4M5wGnpjn837/logiL4dOCjcDm9Nd83cBDwYXjPM4Cbw+2t9p5T7v94tveearX3TNDDc1q4zKz5rGrqe9Y0IiIiEpmqp0REJDIlDRERiUxJQ0REIlPSEBGRyJQ0REQkMiUNaVPM7KGk2UCPMLNb0xTHVWZ2dh3bF5nZ79IQkkgk6nIrbYqZDQJy3H2GmV0D3OXu9c4p0oRxTCaYbfjylO3DgbXuvri5YxKJQnNPSZvi7vOb6txmluPuW/fkHO7+YWPFI9IUVD0lbUpN9ZSZXQ7cFW7zcJmQdNwwM3vRzDaFy1Nm1iNp//Hhe04xs7Fmthn4c7jv+2Y2ycw2mtkqMxtnZvskvXcCcBhwWdK1Lw/31aqeMrMLwgftlJnZEjP7uZnFk/ZfHp7jwPChOyVmNtvMvphynqPN7H9mVhwuH5nZ+Y31u5W2QUlD2qoXgd+Hr48Ml28BhB/w7wDZwFeAy4EDgHFWe3rcBwimcTgrfA3BRHB/Jnh+wdcJHuz1jpnlhfu/RTAX1Pika79YV5BmdjLwBDA1PN9dwA/C86d6lGB+oXMIppB4PHzGBGbWEXgBWACcC5wH/APIr+f3I1InVU9Jm+TuReGMqLj7+ym7bwFWEszNUw5gZtMJPuhPZ8cP+Kfc/acp5/5ezWszywBeIZhpdAzwiLt/YmYlQFEd1071M2CCu9dMQPffMG/90szucPelScf+0d0fDK87BVgFnAHcC+wL5AHXePAwKoCXd3JtkVpU0hCp7USCaaarzSweVgUtJHi05oiUY2uVEMxsVFhNtBaoBLYA7Qk+uCMLE86hwFMpu54g+L97ZMr2bUnA3dcSJKo+4ab5wGbgUTMbUzMLrsiuUtIQqa0r8COC2YCTl4Hs+DwCCL7Nb2Nm/Qg+vA34BsFzHg4n+ADP3o04EqnXSFrvnLJ9Q8p6ec013X09wdTZCeBJoChssxm4izFJG6fqKZHa1hGUNO6vY9+alPXUPuunArnAGHcvAQhLKqkf8FGsIUhW3VK2d0+KMzJ3fw84NekZG38gaAcZtRuxSRulpCFtWU17Rba7lyZtfw0YBkzxXR/IlEPwjOrKpG0XUPv/2rZSQH3cvSpsmzgfuCflfNXAe7sYW815txI06g8Dfrw755C2S0lD2rKa5ylfa2avA8XuPge4leCJZi+a2YME3/h7AycBD7n7hAbO+TpBb6m/m9kDBL2ufkDtqqPZwClmdgqwFlgYtkOkugV4ycz+TvAY0wOB24G/pTSCN8jMvgB8FXgOWBzezzfCeEUiU5uGtGX/A34LXAtMBP4K4O6fElTZbAHuA/4D3AaUAfMaOqG7fwxcAYwk6OL6JYKSwsaUQ+8AZhG0L0wCzqznfC8TPL50BDAOuI6gq/A1u3CfhHE78AuCNpffAP8lSCQikWkaERERiUwlDRERiUxJQ0REIlPSEBGRyJQ0REQkMiUNERGJTElDREQiU9IQEZHIlDRERCSy/w+fUux3YAjkewAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fontsize = {'size' : 15}\n",
    "plt.plot(test_loss)\n",
    "plt.xlabel('iterations', fontdict=fontsize)\n",
    "plt.ylabel('Mean Square Error', fontdict=fontsize)\n",
    "plt.title('MSE VS iterations', fontdict=fontsize)\n",
    "plt.savefig(f'output_prediction_step/{n}ahead/loss_iter.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29710 29995\n",
      "29410 29695\n",
      "29110 29395\n",
      "28810 29095\n",
      "28510 28795\n",
      "28210 28495\n",
      "27910 28195\n",
      "27610 27895\n",
      "27310 27595\n",
      "27010 27295\n",
      "26710 26995\n",
      "26410 26695\n",
      "26110 26395\n",
      "25810 26095\n",
      "25510 25795\n",
      "25210 25495\n",
      "24910 25195\n",
      "24610 24895\n",
      "24310 24595\n",
      "24010 24295\n",
      "23710 23995\n",
      "23410 23695\n",
      "23110 23395\n",
      "22810 23095\n",
      "22510 22795\n",
      "22210 22495\n",
      "21910 22195\n",
      "21610 21895\n",
      "21310 21595\n",
      "21010 21295\n",
      "20710 20995\n",
      "20410 20695\n",
      "20110 20395\n",
      "19810 20095\n",
      "19510 19795\n",
      "19210 19495\n",
      "18910 19195\n",
      "18610 18895\n",
      "18310 18595\n"
     ]
    }
   ],
   "source": [
    "for sample_num in range (1, 40):\n",
    "    y_2 = int (30000 - n - 300 * (sample_num-1))\n",
    "    y_1 = y_2 - 300 + n + 10\n",
    "    print (y_1, y_2)\n",
    "    y = pred_test[y_1:y_2].tolist()\n",
    "    yy = list()\n",
    "    fontsize = {\n",
    "        'size': 20\n",
    "    }\n",
    "    cnt = 0\n",
    "    for _y in y:\n",
    "        if _y == max(y):\n",
    "            cnt = 1\n",
    "        if cnt == 1:\n",
    "            _y = max(y)\n",
    "\n",
    "        yy.append(_y)\n",
    "    del(y)\n",
    "    _y_2 = int (30000 - 300 * (sample_num-1))\n",
    "    _y_1 = _y_2 - 300 + 10\n",
    "    _y = dataset[_y_1:_y_2]\n",
    "    x = np.arange(0,len(_y)).tolist()\n",
    "    _x = np.arange(0,len(_y)).tolist()\n",
    "    plt.figure()\n",
    "    yy = [None] * n + yy\n",
    "    plt.plot(x, yy, 'g', label='prediction')\n",
    "    plt.plot(_x, _y, 'r', label='real')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Prediction of packet loss rate', fontdict=fontsize)\n",
    "    plt.xlabel('number of time slots (time slot is 100s)', fontdict=fontsize)\n",
    "    plt.ylabel('packetloss rate / 100%',fontdict=fontsize)\n",
    "    plt.text(21, 0.3, f'Prediction made {n} slots ahead',fontdict={'size': 13})\n",
    "    plt.savefig(f'output_prediction_step/{n}ahead/test/packetloss_net_{sample_num}_{n}.png',dpi=300)\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    plt.plot(x, yy, 'g', label='prediction')\n",
    "    plt.plot(_x, _y, 'r', label='real')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('enlarged prediction area', fontdict=fontsize)\n",
    "    plt.xlim(250, 300)\n",
    "    plt.xticks(np.arange(250, 300, 5))\n",
    "    plt.grid()\n",
    "    plt.xlabel('number of time slots (time slot is 100s)', fontdict=fontsize)\n",
    "    plt.ylabel('packetloss rate / 100%',fontdict=fontsize)\n",
    "    plt.savefig(\n",
    "        f'output_prediction_step/{n}ahead/test_magnified/packetloss_net_{n}_magnified_sample\\{sample_num}.png',\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.04285714 0.        ]]\n",
      "\n",
      " [[0.         0.04285714]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.04285714 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.05714286 0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.02857143 0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.04285714 0.02857143]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.04285714]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.07142857]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.02857143]]\n",
      "\n",
      " [[0.01428571 0.04285714]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.02857143 0.02857143]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.04285714]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.04285714 0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.04285714 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.04285714 0.        ]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.02857143 0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.04285714]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.02857143 0.        ]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.         0.        ]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.         0.01428571]]\n",
      "\n",
      " [[0.01428571 0.04285714]]\n",
      "\n",
      " [[0.01428571 0.01428571]]\n",
      "\n",
      " [[0.         0.02857143]]\n",
      "\n",
      " [[0.01428571 0.04285714]]\n",
      "\n",
      " [[0.01428571 0.        ]]\n",
      "\n",
      " [[0.         0.04285714]]\n",
      "\n",
      " [[0.1        0.21428572]]\n",
      "\n",
      " [[0.2        0.22857143]]\n",
      "\n",
      " [[0.3        0.45714286]]\n",
      "\n",
      " [[0.3857143  0.55714285]]\n",
      "\n",
      " [[0.44285715 0.6142857 ]]\n",
      "\n",
      " [[0.54285717 0.7285714 ]]\n",
      "\n",
      " [[0.74285716 0.8428571 ]]\n",
      "\n",
      " [[0.74285716 0.87142855]]\n",
      "\n",
      " [[1.         0.87142855]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_pkloss = torch.load(f'./packetloss_net_{n}.pkl')\n",
    "\n",
    "with open (f'../datafile_new/datafile{2}.txt') as _f:\n",
    "        data = _f.read().split('\\n')\n",
    "        data_200 = data[1:-1]\n",
    "final_200 = list()\n",
    "for i in range (int(len(data_200)/100)):\n",
    "    _d = data_200[100*(i-1) : 100*i]\n",
    "    loss_packets = len([_data for _data in _d if _data == 'None'])\n",
    "    final_200.append(loss_packets)\n",
    "final_200 = np.array(final_200)\n",
    "\n",
    "dataset_200 = final_200.astype(np.float32)\n",
    "max_value = np.max(dataset_200)\n",
    "min_value = np.min(dataset_200)\n",
    "scalar = max_value - min_value\n",
    "dataset_200 = list(map(lambda x: x / scalar, dataset_200))\n",
    "model_pkloss.eval()\n",
    "X = np.array(dataset_200)\n",
    "X =X.reshape(-1, 1, n)\n",
    "print (X)\n",
    "data_200 = torch.from_numpy(X)\n",
    "var_200 = Variable(data_200)\n",
    "pred_test_200 = model(var_200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.08894705772399902, 0.08888424187898636]],\n",
       " [[0.037292465567588806, 0.037198565900325775]],\n",
       " [[0.0017609074711799622, 0.0016477704048156738]],\n",
       " [[-0.0031427517533302307, -0.0032586753368377686]],\n",
       " [[0.008047275245189667, 0.00793321430683136]],\n",
       " [[0.014531485736370087, 0.014415796846151352]],\n",
       " [[0.017522893846035004, 0.017405271530151367]],\n",
       " [[0.014564760029315948, 0.01444440707564354]],\n",
       " [[0.014601975679397583, 0.014481335878372192]],\n",
       " [[0.016395509243011475, 0.016275182366371155]],\n",
       " [[0.020382728427648544, 0.020263172686100006]],\n",
       " [[0.03290009871125221, 0.03278356418013573]],\n",
       " [[0.025257743895053864, 0.025136154145002365]],\n",
       " [[0.02111150696873665, 0.020989134907722473]],\n",
       " [[0.018245186656713486, 0.018122762441635132]],\n",
       " [[0.026768725365400314, 0.02664962410926819]],\n",
       " [[0.024199962615966797, 0.024078823626041412]],\n",
       " [[0.01723601669073105, 0.017112955451011658]],\n",
       " [[0.021026503294706345, 0.02090570703148842]],\n",
       " [[0.016310494393110275, 0.016188085079193115]],\n",
       " [[0.015579037368297577, 0.01545717567205429]],\n",
       " [[0.022240370512008667, 0.022120889276266098]],\n",
       " [[0.021526582539081573, 0.021405860781669617]],\n",
       " [[0.02177564427256584, 0.021654777228832245]],\n",
       " [[0.027480684220790863, 0.02736138179898262]],\n",
       " [[0.021014228463172913, 0.020891934633255005]],\n",
       " [[0.021915901452302933, 0.02179461345076561]],\n",
       " [[0.027426976710557938, 0.027307406067848206]],\n",
       " [[0.022832900285720825, 0.02271101623773575]],\n",
       " [[0.022305510938167572, 0.022183962166309357]],\n",
       " [[0.020186297595500946, 0.020064368844032288]],\n",
       " [[0.031237538903951645, 0.031119443476200104]],\n",
       " [[0.0231875441968441, 0.02306518703699112]],\n",
       " [[0.024347905069589615, 0.024226687848567963]],\n",
       " [[0.02229677140712738, 0.02217496931552887]],\n",
       " [[0.027733713388442993, 0.027613934129476547]],\n",
       " [[0.017704688012599945, 0.01758120208978653]],\n",
       " [[0.023185070604085922, 0.023064620792865753]],\n",
       " [[0.016670186072587967, 0.016547389328479767]],\n",
       " [[0.032781604677438736, 0.032664768397808075]],\n",
       " [[0.018092241138219833, 0.017968475818634033]],\n",
       " [[0.03553830087184906, 0.03542153164744377]],\n",
       " [[0.02920619025826454, 0.029084697365760803]],\n",
       " [[0.0185173898935318, 0.01839318871498108]],\n",
       " [[0.04676717892289162, 0.046653639525175095]],\n",
       " [[0.021682240068912506, 0.021556615829467773]],\n",
       " [[0.01748628169298172, 0.017362289130687714]],\n",
       " [[0.02460109069943428, 0.02448078617453575]],\n",
       " [[0.02210734784603119, 0.02198570966720581]],\n",
       " [[0.02201823890209198, 0.02189689129590988]],\n",
       " [[0.016695775091648102, 0.016573019325733185]],\n",
       " [[0.024632681161165237, 0.024513252079486847]],\n",
       " [[0.018518127501010895, 0.018395833671092987]],\n",
       " [[0.021324895322322845, 0.021204151213169098]],\n",
       " [[0.016385845839977264, 0.016263402998447418]],\n",
       " [[0.026464134454727173, 0.026345517486333847]],\n",
       " [[0.020547453314065933, 0.02042551338672638]],\n",
       " [[0.016593266278505325, 0.016470737755298615]],\n",
       " [[0.020719293504953384, 0.02059878408908844]],\n",
       " [[0.017912831157445908, 0.017791077494621277]],\n",
       " [[0.028721336275339127, 0.02860325574874878]],\n",
       " [[0.036565784364938736, 0.03644822537899017]],\n",
       " [[0.024443581700325012, 0.024320334196090698]],\n",
       " [[0.019381966441869736, 0.019258640706539154]],\n",
       " [[0.021449346095323563, 0.02132789045572281]],\n",
       " [[0.023501187562942505, 0.023380465805530548]],\n",
       " [[0.027778994292020798, 0.02765928953886032]],\n",
       " [[0.02857963740825653, 0.02845929190516472]],\n",
       " [[0.023285023868083954, 0.023162677884101868]],\n",
       " [[0.024266619235277176, 0.024145308881998062]],\n",
       " [[0.02232068032026291, 0.022198878228664398]],\n",
       " [[0.01686108484864235, 0.016738127917051315]],\n",
       " [[0.022761307656764984, 0.02264115959405899]],\n",
       " [[0.018243737518787384, 0.018121622502803802]],\n",
       " [[0.017693065106868744, 0.017571397125720978]],\n",
       " [[0.017339643090963364, 0.017218150198459625]],\n",
       " [[0.015606909990310669, 0.01548510417342186]],\n",
       " [[0.024170048534870148, 0.024051200598478317]],\n",
       " [[0.021901816129684448, 0.021780874580144882]],\n",
       " [[0.031698886305093765, 0.03158088028430939]],\n",
       " [[0.01986711099743843, 0.019743770360946655]],\n",
       " [[0.023719485849142075, 0.023598667234182358]],\n",
       " [[0.023816213011741638, 0.023695170879364014]],\n",
       " [[0.03407852351665497, 0.03396056592464447]],\n",
       " [[0.022036820650100708, 0.02191340923309326]],\n",
       " [[0.018831893801689148, 0.018709007650613785]],\n",
       " [[0.024918217211961746, 0.02479812502861023]],\n",
       " [[0.02978987619280815, 0.02967056632041931]],\n",
       " [[0.023269325494766235, 0.02314700558781624]],\n",
       " [[0.024293988943099976, 0.024172719568014145]],\n",
       " [[0.02051277458667755, 0.02039046585559845]],\n",
       " [[0.01647115871310234, 0.016348402947187424]],\n",
       " [[0.024452954530715942, 0.024333514273166656]],\n",
       " [[0.023794762790203094, 0.02367403730750084]],\n",
       " [[0.02229372039437294, 0.022172346711158752]],\n",
       " [[0.027670852839946747, 0.02755124494433403]],\n",
       " [[0.022774092853069305, 0.02265210822224617]],\n",
       " [[0.024095680564641953, 0.023974627256393433]],\n",
       " [[0.017070554196834564, 0.016947433352470398]],\n",
       " [[0.017519645392894745, 0.017397835850715637]],\n",
       " [[0.020772799849510193, 0.02065229043364525]],\n",
       " [[0.027117863297462463, 0.026998870074748993]],\n",
       " [[0.022717196494340897, 0.0225955992937088]],\n",
       " [[0.017056390643119812, 0.016933511942625046]],\n",
       " [[0.020990855991840363, 0.020870167762041092]],\n",
       " [[0.017893806099891663, 0.017771899700164795]],\n",
       " [[0.015828214585781097, 0.015706177800893784]],\n",
       " [[0.015294760465621948, 0.015173070132732391]],\n",
       " [[0.015140697360038757, 0.015019245445728302]],\n",
       " [[0.020298056304454803, 0.02017829939723015]],\n",
       " [[0.023008357733488083, 0.022888559848070145]],\n",
       " [[0.016760356724262238, 0.016637973487377167]],\n",
       " [[0.017395280301570892, 0.017273854464292526]],\n",
       " [[0.017238721251487732, 0.01711738109588623]],\n",
       " [[0.015585049986839294, 0.015463333576917648]],\n",
       " [[0.024160999804735184, 0.024042200297117233]],\n",
       " [[0.027520399540662766, 0.027401171624660492]],\n",
       " [[0.022932618856430054, 0.0228109247982502]],\n",
       " [[0.024154331535100937, 0.024033386260271072]],\n",
       " [[0.025993533432483673, 0.025873009115457535]],\n",
       " [[0.01744893565773964, 0.017325613647699356]],\n",
       " [[0.03730091452598572, 0.03718506172299385]],\n",
       " [[0.020392779260873795, 0.02026868611574173]],\n",
       " [[0.016924526542425156, 0.01680132746696472]],\n",
       " [[0.020738840103149414, 0.020617909729480743]],\n",
       " [[0.021463871002197266, 0.021342962980270386]],\n",
       " [[0.01657230779528618, 0.01644985005259514]],\n",
       " [[0.019057683646678925, 0.01893674209713936]],\n",
       " [[0.019257057458162308, 0.019136011600494385]],\n",
       " [[0.01941414177417755, 0.019293084740638733]],\n",
       " [[0.016018547117710114, 0.01589645817875862]],\n",
       " [[0.02623594179749489, 0.026117511093616486]],\n",
       " [[0.017166152596473694, 0.01704341173171997]],\n",
       " [[0.021176062524318695, 0.021055519580841064]],\n",
       " [[0.021476346999406815, 0.02135547623038292]],\n",
       " [[0.035747162997722626, 0.03563057631254196]],\n",
       " [[0.02572430670261383, 0.02560192346572876]],\n",
       " [[0.028820261359214783, 0.028699640184640884]],\n",
       " [[0.037065550684928894, 0.03694706782698631]],\n",
       " [[0.02102506533265114, 0.020900383591651917]],\n",
       " [[0.03412114083766937, 0.034002549946308136]],\n",
       " [[0.1544579714536667, 0.154373437166214]],\n",
       " [[0.19879424571990967, 0.19868767261505127]],\n",
       " [[0.37354904413223267, 0.3734349012374878]],\n",
       " [[0.46314889192581177, 0.4629770517349243]],\n",
       " [[0.5290510058403015, 0.5288301110267639]],\n",
       " [[0.5983486175537109, 0.5980859398841858]],\n",
       " [[0.6715322136878967, 0.671230137348175]],\n",
       " [[0.7044243216514587, 0.7040886878967285]],\n",
       " [[0.7436950206756592, 0.7433348298072815]]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_200.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0429, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0429]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0429, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0571, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0429, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0429]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0714]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0286]],\n",
       "\n",
       "        [[0.0143, 0.0429]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0286, 0.0286]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0429]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0429, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0429, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0429, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0286, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0429]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0286, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0143]],\n",
       "\n",
       "        [[0.0143, 0.0429]],\n",
       "\n",
       "        [[0.0143, 0.0143]],\n",
       "\n",
       "        [[0.0000, 0.0286]],\n",
       "\n",
       "        [[0.0143, 0.0429]],\n",
       "\n",
       "        [[0.0143, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0429]],\n",
       "\n",
       "        [[0.1000, 0.2143]],\n",
       "\n",
       "        [[0.2000, 0.2286]],\n",
       "\n",
       "        [[0.3000, 0.4571]],\n",
       "\n",
       "        [[0.3857, 0.5571]],\n",
       "\n",
       "        [[0.4429, 0.6143]],\n",
       "\n",
       "        [[0.5429, 0.7286]],\n",
       "\n",
       "        [[0.7429, 0.8429]],\n",
       "\n",
       "        [[0.7429, 0.8714]],\n",
       "\n",
       "        [[1.0000, 0.8714]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = np.array(pred_test_200.tolist())\n",
    "_orig = np.array(data_200)\n",
    "\n",
    "plot_table = {\n",
    "    'original': _orig,\n",
    "    'prediction': _pred\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "for test_num in range (1,2):\n",
    "    model_pkloss = torch.load(f'./packetloss_net_{n}.pkl')\n",
    "\n",
    "    with open (f'../datafile_new/datafile{test_num}.txt') as _f:\n",
    "            data = _f.read().split('\\n')\n",
    "            data_200 = data[1:-1]\n",
    "    final_200 = list()\n",
    "    for i in range (int(len(data_200)/100)):\n",
    "        _d = data_200[100*(i-1) : 100*i]\n",
    "        loss_packets = len([_data for _data in _d if _data == 'None'])\n",
    "        final_200.append(loss_packets)\n",
    "    final_200 = np.array(final_200)\n",
    "\n",
    "    dataset_200 = final_200.astype(np.float32)\n",
    "    max_value = np.max(dataset_200)\n",
    "    min_value = np.min(dataset_200)\n",
    "    scalar = max_value - min_value\n",
    "    dataset_200 = list(map(lambda x: x / scalar, dataset_200))\n",
    "    X, Y = create_dataset(dataset_200)\n",
    "    print (n)\n",
    "    model_pkloss.eval()\n",
    "    X =X.reshape(-1, 1, n)\n",
    "    data_200 = torch.from_numpy(X)\n",
    "    var_200 = Variable(data_200)\n",
    "    print (len(var_200))\n",
    "    pred_test_200 = model(var_200)\n",
    "    pred_test_200 = pred_test_200.view(-1).data.numpy()\n",
    "\n",
    "    # 画出实际结果和预测的结果\n",
    "    pred_test_200 = [None] * n + list(pred_test)\n",
    "    plt.plot(pred_test_200, 'g', label='prediction')\n",
    "    plt.plot(dataset_200, 'b', label='real')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Generalization Test of model')\n",
    "    plt.xlabel('number of time-period (100s)')\n",
    "    plt.ylabel('rate of packet loss (*100%)')\n",
    "    plt.savefig(f'output_prediction_step/{n}ahead/Generalisation/Generalization test sample_{n}_{test_num}', dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
